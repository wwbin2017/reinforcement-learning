<center><h2>TRPO</h2></center>

​	策略梯度的方法最大的问题在于步长的问题，太大容易发散，太小容易更新缓慢。针对这个问题，当然有很多解决方法，但是这些方法在一定程度上避免直接利用步长。TRPO没有选择避免更新步长的问题，而是正面解决了这个问题。TRPO(Trust Region Policy Optimization)，中文”置信域策略优化“。

​	根据策略梯度方法，参数更新方式为：

​					$$\theta_{new}=\theta_{old} + \alpha \nabla_{\theta}J$$	

​	策略梯度算法的硬伤在于步长更新$\alpha$，当学习到一个不好的策略的时候，容易越学越差，最后崩溃。那么什么才算是合适步长，我们可以认为至少能保证更新参数后，回报函数不能更差。我们知道回报函数表达式为：

​					$$\eta(\hat \pi) = E_{\tau|\pi}[\sum_{t=0}^{\infty}\gamma^{t}(r(s_t))]$$

​	TRPO的想法是能够找到一个新策略使得回报函数单调不减。很自然的想法就是把回报函数分解成旧策略加上其他项，如果能够保证其他项非负，那就就保证单调不减，问题这这个其他项是否存在，在2002年Sham Kakade就已经提出来，如下式：

​				$$\eta(\hat \pi) =\eta(\pi)  +  E_{s_0,a_0,…\sim \hat\tau}[\sum_{t=0}^{\infty}\gamma^{t}A_{\pi}(s_t,a_t)]​$$

其中：

​		$$A_{\pi}(s,a) = Q_{\pi}(s,a) - V_{\pi}(a)=E_{s^{'} \sim P(s^{'}|s,a)}[r(s)+\gamma V_{\pi}(s^{'}) - V_{\pi}(s)]​$$

我们称之为优势函数，非常直观的理解就是，在当前转态s下，执行a动作的回报相对于平均回报的差值。

证明如下：

​				$$ E_{s_0,a_0,…\sim \hat\tau}[\sum_{t=0}^{\infty}\gamma^{t}A_{\pi}(s_t,a_t)]	​$$

​				$$= E_{s_0,a_0,…\sim \hat\tau}[\sum_{t=0}^{\infty}\gamma^{t}(r(s_t)+\gamma V_{\pi}(s_{t+1}) - V_{\pi}(s_t))]$$

​				$$= E_{s_0,a_0,…\sim \hat\tau}[\sum_{t=0}^{\infty}\gamma^{t}(r(s_t))+\sum_{t=0}^{\infty}\gamma^{t}(\gamma V_{\pi}(s_{t+1}) - V_{\pi}(s_t))]$$

​				$$= E_{s_0,a_0,…\sim \hat\tau}[\sum_{t=0}^{\infty}\gamma^{t}(r(s_t))]+ E_{s_0}[- V_{\pi}(s_0)]$$

​				$$=\eta(\hat \pi) - \eta(\pi)$$				

我们把上面的公式 中的期望展开看下：

​			$$\eta(\hat \pi) =\eta(\pi)  + \sum_{t=0}^{\infty}\sum_{s}P(s_t=s|\hat \pi) \sum_{a} \hat \pi(a|s) \gamma^{t}A_{\pi}(s,a)]​$$

现在我们定义 $\rho_{\pi}(s)=P(s_0=s) + \gamma P(s_1=s) + \gamma^{2}P(s_2=s) + \gamma^{3}P(s_3=s)  + ... ​$

我们称这个为未归一化折扣概率。

​		$$\eta(\hat \pi) =\eta(\pi)  +\sum_{s}\rho_{\pi}(s|\hat \pi) \sum_{a} \hat \pi(a|s) A_{\pi}(s,a)]$$

上面的公式还无法使用代码实现，而且加的其他 想 里面既有旧策略又有新策略，还是前面的思想，可以用旧策略近似新策略。我们来看下TRPO使用什么方式简化上式。

1. 忽略状态分布的变化，使用旧策略代替新策略的状态分布。如果两个策略比较相近的话，没毛病。

   $$L_{\pi}(\hat \pi) =\eta(\pi)  +\sum_{s}\rho_{\pi}(s| \pi) \sum_{a} \hat \pi(a|s) A_{\pi}(s,a)]​$$

2. 上面的公式还存在新策略，我们使用重要性采样处理动作分布

   ​				$$\sum_{a} \hat \pi(a|s) A_{\pi}(s,a)] = E_{a \sim  q}[\frac{\hat \pi_{\theta}(a|s)}{q(a|s)}A_{\pi}(s,a)]$$

   我们取$q(a|s)=\pi(a|s)$，在利用 $\frac{1}{1-\gamma}E_{s \sim \rho_{\pi}}[...]$代替$\sum_{s}\rho_{\pi}[…]$，因此，可以在简化一下

   ​				$$L_{\pi}(\hat \pi) =\eta(\pi)  +  E_{s \sim \rho_{\pi},a \sim  \pi}[\frac{\hat \pi_{\theta}(a|s)}{\pi(a|s)}A_{\pi}(s,a)]​$$

   我们比较$\eta(\hat \pi)​$和$L_{\pi}(\hat \pi) ​$的区别，到目前为止，我们只是对状态分布做了一个近似，因此它们之间的就在于分布的区别，这两个回报函数都是关于$\hat \pi​$的函数，我们看下在$\pi​$处的一阶近似，即

   ​		$$L_{\pi}( \pi) =\eta(\pi)​$$

   ​		$$\nabla_{\pi}L_{\pi}(\pi)|_{\hat \pi = \pi} = \nabla_{\pi} \eta(\pi)|_{\hat \pi = \pi}​$$

   因此，在$\pi​$附近，能改善L的策略也能改善源回报函数。问题是步长应该取多大，这里我们引入第二个重要不等式：

   ​			$$\eta(\hat \pi ) >= L_{\pi}(\hat \pi)  - CD_{KL}^{max}(\pi, \hat \pi)​$$

   ​			$$C=\frac{2\epsilon \gamma}{(1-\gamma)^2}​$$

   当然上面的证明很复杂，这里就不展开，我们来看下这个不等式说明了啥，这里给出了回报的下界，定义下界为$M_i(\pi)= L_{\pi}(\hat \pi)  - CD_{KL}^{max}(\pi, \hat \pi)​$ .我们利用这个来正面下策略的单调性。证明：

   ​	$$\eta(\pi_{i+1} ) >=  M_i(\pi_{i+1}) 且 \eta(\pi_i)=M_i(\pi_i)​$$

   ​        因此，$\eta(\pi_{i+1} )- \eta(\pi_i) >=  M_i(\pi_{i+1}) -M_i(\pi_i)​$，只要使得$ M_i(\pi_{i+1})​$ 最大，就能保证大于0，所以我们只要优化这个：

   ​	$$max M_i(\pi_{i+1}) = max_{\hat \pi}[ L_{\pi}(\hat \pi)  - CD_{KL}^{max}(\pi, \hat \pi)]​$$

   利用惩罚因子$C$，每次迭代很小的步长，因此问题就转化成：

   ​				$$max  E_{s \sim \rho_{\pi},a \sim  \pi}[\frac{\hat \pi_{\theta}(a|s)}{\pi(a|s)}A_{\pi}(s,a)]​$$ 

   ​				$$where D_{KL}^{max}(\pi, \hat \pi) <= \delta​$$

3. 在约束条件中  ，使用平均KL散度代替最大散度。

   ​				$$where D_{KL}^{-}(\pi, \hat \pi) <= \delta​$$

4. 目标 函数 进行一阶 逼近 ，约束 条件进行二阶逼近。

   ​		$$max  E_{s \sim \pi,a \sim  \pi}[\frac{\hat \pi_{\theta}(a|s)}{\pi(a|s)}A_{\pi}(s,a)]​$$ 

   ​		$$where E_{s \sim \pi}[ D_{KL}(\pi, \hat \pi) ]<= \delta$$

   上式仍然很难写成代码的形式，因此进一步简化

   ​		$$min_{\pi} -[\nabla_{\hat \pi} L_{\pi}(\hat \pi)|_{\hat \pi = \pi} *(\hat \pi - \pi)]​$$

   ​		$$st:   \frac{1}{2}(\pi - \hat \pi)^{T}A(\pi)(\hat \pi - \pi) <= \delta$$

