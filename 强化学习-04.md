<center><h2>策略梯度方法</h2></center>
​	初识强化学习的时候，只知道有DQN，其实DQN只是强化学习的一部分，主要代表是DeepMind。强化学习里面最重要的就是策略搜索的方法，主要代表是OpenAI。

​	策略搜索是指将策略用函数来表示$\pi_{\theta}(s)$:可以选择线性或者非线性函数表示，寻找最优的参数$\theta$，使得累计回报最大：$$E[\sum_{t=0}^{T}R(s_{t}|\pi_\theta)]$$。

​	我们比较一下值函数 和 策略搜索的优缺点。

1. 策略搜索是直接对策略优化，形式更简单，有更好的收敛性；
2. 值函数方法对动作空间要求很高；
3. 策略搜索直接使用随机策略，因为随机策略可以将探索直接集成到策略学习里面去；
4. 策略搜索容易陷入局部最小值；
5. 评估策略是方差较大。

现在我们推导一下策略梯度方法：

​	我们用$$\tau$$来表示一组状态-行为序列，其实可以理解为就是一个episode，$s_0,u_0,…,s_T,u_T$,符号$$R(\tau)=\sum_{t=0}^{T}(s_t,u_t)$$表示序列的回报，$P(\tau,\theta)$表示序列出现的概率，因此目标函数可以如下：

​				$$U(\theta)=E[\sum_{t=0}^{T}R(s_{t}|\pi_\theta)]=\sum_{\tau}P(\tau,\theta)R(\tau)$$

有了目标函数就好办了，这个问题就变成优化问题了，优化方法我们有很多。我们先使用下梯度方法。如下：

​				$$\nabla U(\theta)=\nabla \sum_{\tau}P(\tau,\theta)R(\tau) $$

​						$$ =\sum_{\tau}\nabla P(\tau,\theta)R(\tau) $$

​						$$ =\sum_{\tau}P(\tau,\theta) \frac{\nabla P(\tau,\theta)}{P(\tau,\theta) }R(\tau) $$

​						$$=\sum_{\tau}P(\tau,\theta)\nabla logP(\tau,\theta)R(\tau)$$

​	我们做一下变换之后，最终的梯度变成了一个期望，当然之所以要这么变换，主要是因为概率$P(\tau,\theta)$是一个乘积，取$log$之后就变成了求和，方便计算梯度。因此我们需要采样，假设使用当前策略$\pi_\theta$采样m条序列，因此策略梯度的估计可以使用如下方式计算。

​				 $$\nabla U(\theta)  \approx \hat g = \frac{1}{m}\sum_{i=1}^{m}\nabla logP(\tau,\theta)R(\tau)$$

​	我们知道梯度是变化趋势最陡的地方，因此$\nabla logP(\tau,\theta)$ 参数表示如果沿正方向，这序列概率会增大，反之亦然。$ R(\tau)$ 控制着更新的方向和步长，解释和$\nabla logP(\tau,\theta)$一样。

​	上面的是式子最难计算的还是概率，回报只要直接累计求和就可以。因此，我们把概率展开看下。

​				$$\nabla_{\theta} logP(\tau^{i},\theta)=\nabla_{\theta}log[\prod_{t=0}^{T}P(s_{t+1}^{i}|s_{t}^{i},u_{t}^{i})*\pi_{\theta}(u_{t}^{i}|s_{t}^{i})]$$

​					$$\sum_{t=0}^{T}  \nabla_{\theta} log \pi_{\theta}(u_{t}^{i}|s_{t}^{i})$$

​	上面的公式把联乘展开后，因为我们是对$$\theta$$求偏导，因此不含$\theta$的都去掉，最后只剩下上式。

​	上面的策略梯度是无偏的，但是方差很大，因此我们引入一个常数，在保证梯度不变的情况下，即

​		 $$\nabla U(\theta)  \approx \hat g = \frac{1}{m}\sum_{i=1}^{m}\nabla logP(\tau,\theta)R(\tau) = \frac{1}{m}\sum_{i=1}^{m}\nabla logP(\tau,\theta)(R(\tau)-b) $$

​	我们引入上常数$b$是为了降低方差，我们都知道方差的公式 ，因此只要求方差对参数$b$的偏导即可：

$$min\frac{\partial Var(x)}{\partial  b} $$  可以求出$b$的值。

​	从 MDP的假设我们知道，当前的动作不能够影响已经发生的回报，因此，我们再把回报累计求和去掉其中的相对于动作之前的回答，因此策略梯度如下：

​		 $$\nabla U(\theta)  \approx  \frac{1}{m}\sum_{i=1}^{m}\nabla logP(\tau,\theta)R(\tau) = \frac{1}{m}\sum_{i=1}^{m}\nabla logP(\tau,\theta)(R(\tau)-b) $$

​				$$=\frac{1}{m}\sum_{i=1}^{m} \sum_{t=0}^{T-1}\nabla logP(\tau,\theta)(\sum_{k=t}^{T-1}R(s_{k}^{i})-b)$$	

​	这一节我们主要介绍了基本的策略梯度，光看上面公式并没有什么意思，我们在这举个例子，看下实际是怎么把上面一堆公式变成代码的。

​	还是第三章的例子， 源码见[github](https://github.com/wwbin2017/reinforcement-learning/blob/master/src/third/policy_network.py)

![强化学习基本框架](image/image-03-02.png)
