<center><h2>DQN</h2></center>

### 基于值函数逼近

​	前一节我们介绍了MC和TD方法，我们很容易看出这些方法需要状态空间和动作空间是离散的，而且还不能太大。其实是基于表格的强化学习，状态-动作对。

​	我们再回顾一下MC和TD的更新公式：

​	MC：

​                  $$\begin{aligned} q_k(s,a) =q_{k-1}(s,a)+\alpha(G_k(s,a)-q_{k-1}(s,a)) \end{aligned}$$

​	TD:

​          	$$ q_{k}(s,a) = q_{k-1}(s,a) + \alpha (R_{k}(s,a) + \gamma q_{k}(s_{t+1},a_{t+1})-q_{k-1}(s,a))$$   

​	从状态值函数的更新过程可以产出，值函数是不断的去逼近目标函数的。

![强化学习基本框架](image/image-03-01.png)

​	函数逼近$\hat{v}(s,\theta)$的过程是一个监督学习的过程，其数据标签对$(S_t, U_t)$，这里的$U_t$可以对应上面公式的不同的值。因此我们可以设置训练目标函数为：

​					$$arg   min(q(s,a)-\hat{q}(s,a,\theta))^2$$

​	现在我们来看如何学习他，我们可以很容易的使用梯度下降的方法学习。

​				$$\theta_{t+1}=\theta_{t} + \alpha[U_t - \hat{v}(S_t, \theta_t)]\nabla\hat{v}(S_t, \theta_t)​$$

​	由前面介绍，产生一次试验的过程(MC)，其实是一个监督学习的过程，我们可以得到数据集$<s_1,G_1>, <s_2, G_2>, …, <s_t, G_t>$。

​	因此，

​				$$\nabla\theta = \alpha[G_t - \hat{v}(S_t, \theta)]\nabla_{\theta}\hat{v}(S_t, \theta_t)​$$

​	我们再看时间差分，TD(0)方法中的目标值函数为$U_t = R_{t+1} + \gamma\hat{v}(s_{t+1}, \theta)$，目标值中用到了bootstrapping，我们可以看到参数$\theta$即出现在了要估计的值函数里面也出现在目标值函数中，因此，我们称基于半梯度TD(0)的值函数评估方法。

​			$$\theta_{t+1}=\theta_{t} + \alpha[R  + \gamma\hat{v}(S_{t+1},\theta)- \hat{v}(S_t, \theta_t)]\nabla\hat{v}(S_t, \theta_t)​$$

​	当然到目前为止，我们瞎聊了这么多，各种公式堆积，但是我们没有说要逼近的值函数的形式。其实这个地方是随意定义的。比如对于线性逼近，你可以使用多项式函数，傅里叶函数，径向基函数等。非线性逼近可以选择我们都熟知的拟合效果非常好的神经网络。

###  DQN

​	强化学习的火爆必须归功于Google的DeepMind公司。Qlearning是1989年提出，2015年Nature论文提到的DQN是在Qlearning基础上修改得到的，主要体现一下几个方面。

  1. DQN使用CNN逼近值函数；(FlappyBird举例)
      ![强化学习基本框架](image/image-03-02.png)

  2. DQN使用经验回放训练强化学习；

       1. 网络不稳定、不收敛

       2. 为什么可以解决问题1：因为训练网路的时候，加上数据是独立同分布的，但是强化学习的样本是存在关联的，因此训练网络当然不稳定。经验回放可以打破数据之间的关联。

          | <$s_1, a_1,r_2,s_2$> |
          | -------------------- |
          | <$s_2, a_2,r_3,s_3$> |
          | <$s_3, a_3,r_4,s_4$> |
          | ……….                 |

          强化学习的过程中，我们将样本数据存储到一个数据中，然后随机均匀采样，训练DNN。

  3. DQN独立设置了目标网络来单独处理时间查分算法中的TD偏差。

     $$\theta_{t+1}=\theta_{t} + \alpha[R  + \gamma maxt Q(S_{t+1}, a_{t+1},\theta^-)-Q(S_t, a_t,\theta_t)]\nabla Q(S_t,,a_t, \theta_t)​$$

     我们称$R  + \gamma maxt Q(S_{t+1}, a_{t+1},\theta^-)$目标网络，DQN出现之前，目标网络和值函数逼近网络是同一个网络，这样容易导致数据建存在关联性，从而使得训练不稳定。为了解决这个问题DeepMind提出单独设置一个目标网络。值函数逼近网络每步都更新，但是目标网络固定步数更新一次。

  4. 伪代码如下：
      ![强化学习基本框架](image/image-03-03.png)

  5. 例子  [github](https://github.com/wwbin2017/reinforcement-learning/blob/master/src/third/deep_q_network.py)  

      ```python
              if t > Config.OBSERVE:
                  minibatch = random.sample(D, Config.BATCH)
                  s_j_batch = [d[0] for d in minibatch]
                  a_batch = [d[1] for d in minibatch]
                  r_batch = [d[2] for d in minibatch]
                  s_j1_batch = [d[3] for d in minibatch]
      
                  y_batch = []
                  readout_j1_batch = q_value_evaluation.predict(sess, s_j1_batch)
                  for i in range(0, len(minibatch)):
                      terminal = minibatch[i][4]
                      if terminal:
                          y_batch.append(r_batch[i])
                      else:
                          y_batch.append(r_batch[i] + Config.GAMMA * \
                                         np.max(readout_j1_batch[i]))
                  q_value_evaluation.train(sess, s_j_batch, a_batch, y_batch)
      
              s_t = s_t1
              t += 1
              if t > Config.OBSERVE:
                  if t % Config.UPDATE_TARGET_ESTIMATOR_EVERY == 0 and fix_target:
                      sess.run(assign_ops)
      ```

      

  6. 相关改进

      1. Double DQN

         1. 虽然DQN克服了很多问题，但是DQN没有解决QLearning过估计的缺点。过估计是指估计值比实际值大，主要在于max操作。如果过估计是均匀的，显然不会响应我们最终的结果的，因为我们并没有使用它的值。但是实际情况不一定均匀。因此使用另外一个网络评估它；

            $$Y_{t}^{Double} = R_{t+1} + \gamma Q(S_{t+1},argmaxQ(S_{t+1},a;\theta_t^-);\theta_t )$$

      2. Prioritized DQN

         1. DQN的成功归于经验回放-设置独立目标网络。但是经验回放采用的均匀分布。因此，我们可以采用我们可以重要性采用。重要性显然可以使用差值比较大赋予比较高的值。

      3. Dueling DQN(后面介绍，涉及后面的知识)

  7. 总结

        ​	本节介绍了函数逼近思想，然后介绍了DQN方法和实例。到目前为止我们介绍的方法都是如何计算Q值函数。下一节我们换一种思路，可不可以直接估计策略函数 $\pi​$。
