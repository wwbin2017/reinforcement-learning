<center><h2>PPO</h2></center>
​	PPO（Proximal Policy Optimization）的动机与TRPO相同：我们如何才能使用当前拥有的数据在策略上采取最大可能的改进，而又不会走得太远而导致意外导致性能下降呢？在TRPO尝试使用复杂的二阶方法解决此问题的地方，PPO是一阶方法的族，它们使用其他一些技巧来使新策略接近于旧策略。 PPO方法明显更易于实现，并且从经验上看，其性能至少与TRPO相同。

​	PPO有两种主要变体：**PPO-Penalty**和**PPO-Clip**。

​	PPO-Penalty近似解决了TRPO之类的受KL约束的更新，但是惩罚了目标函数中的KL背离而不是使其成为硬约束，并且在训练过程中自动调整了罚分系数，以使其适当地缩放。

​	PPO-Clip在目标中没有KL散度项，也没有任何约束。取而代之的是依靠对目标函数的专门裁剪来消除新策略远离旧策略的动机。

​	在这里，我们仅关注PPO-Clip（OpenAI使用的主要变体）。

1. PPO是一种基于策略的算法。
2. PPO可用于具有离散或连续动作空间的环境。
3. PPO的Spinning Up实现支持与MPI并行化。

PPO剪辑更新策略通过

$$\theta_{k+1} = \arg \max_{\theta} \underset{s,a \sim \pi_{\theta_k}}{{\mathrm E}}\left[     L(s,a,\theta_k, \theta)\right]$$

通常采取多个步骤（通常是小批量）SGD来最大化目标。 这里L是由:

$$L(s,a,\theta_k,\theta) = \min\left( \frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}  A^{\pi_{\theta_k}}(s,a), \;\; \text{clip}\left(\frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}, 1 - \epsilon, 1+\epsilon \right) A^{\pi_{\theta_k}}(s,a) \right)$$

其中$\epsilon$是一个（小）超参数，它粗略地说出了新策略与旧策略的距离。

这是一个非常复杂的表述，很难一眼就知道它在做什么，或者它如何帮助使新策略接近旧策略。 事实证明，此目标有一个相当简化的版本，它易于处理（也是我们在代码中实现的版本）：

$$L(s,a,\theta_k,\theta) = \min\left( \frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}  A^{\pi_{\theta_k}}(s,a), \;\; g(\epsilon, A^{\pi_{\theta_k}}(s,a)) \right)$$

where

$$g(\epsilon, A) = \left\{     \begin{array}{ll}     (1 + \epsilon) A & A \geq 0 \\     (1 - \epsilon) A & A < 0.     \end{array}     \right.$$

为了弄清楚从中得到的直觉，让我们看一个状态对（s，a），并考虑案例。

优势是肯定的：假设该状态-动作对的优势是积极的，在这种情况下，它对目标的贡献减少为:

$$L(s,a,\theta_k,\theta) = \min\left( \frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}, (1 + \epsilon) \right)  A^{\pi_{\theta_k}}(s,a)$$

因为优势是肯定的，所以如果采取行动的可能性更大，也就是说，如果$\pi_ {\theta}(a | s)$增加，则目标也会增加。 但是此术语中的最小值限制了目标可以增加多少。 一旦$\pi _{\theta}(a | s)>(1+ \epsilon)\pi _{\theta_k}(a | s)$，最小值就会增加，此项达到$(1+ \epsilon)A ^ {\pi_{\theta_k}}(s，a)$。 因此：远离旧策略不会使新策略受益。

优势为负：假设该状态对对的优势为负，在这种情况下，其对目标的贡献减少为:

$$L(s,a,\theta_k,\theta) = \max\left( \frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}, (1 - \epsilon) \right)  A^{\pi_{\theta_k}}(s,a)$$

因为优势是负面的，所以如果行动变得不太可能（即$\pi_{\theta}(a | s)$减小），则目标将增加。 但是此术语中的最大值限制了可以增加多少目标。 一旦$\pi_{\theta}(a | s)<(1- \epsilon)\pi_{\theta_k}(a | s)$，最大值就会增加，此项达到$(1- \epsilon)A^{\pi_{\theta_k}}(s，a)$。 因此，再次：新策略不会因远离旧策略而受益。

尽管这种削减对确保合理的策略更新大有帮助，但仍然有可能最终产生与旧策略相距太远的新策略，并且不同的PPO实现使用很多技巧来避免这种情况。 在此处的实现中，我们使用一种特别简单的方法：提前停止。 如果新策略与旧策略的平均KL差距超出阈值，我们将停止采取梯度步骤。

Exploration vs. Exploitation
PPO以一种按策略方式训练随机策略。 这意味着它会根据其随机策略的最新版本通过采样操作来进行探索。 动作选择的随机性取决于初始条件和训练程序。 在训练过程中，由于更新规则鼓励该策略利用已经发现的奖励，因此该策略通常变得越来越少随机性。 这可能会导致策略陷入局部最优状态。

伪码:

![](image/image-07-01.svg)

还是第三章的例子， 源码见[github](https://github.com/wwbin2017/reinforcement-learning/blob/master/src/third/ppo_network.py)

![[强化学习基本框架]](image/image-03-02.png)

[openAI ppo 介绍](https://openai.com/blog/openai-baselines-ppo/)

